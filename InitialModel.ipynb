{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd53b9d",
   "metadata": {},
   "source": [
    "# --> 1.) Initial LLM: Without Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ae906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d52a790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db720358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwen_chatbot_stream(messages, max_new_tokens=512, temperature=0.7, top_k=50, top_p=0.95):\n",
    "\n",
    "    \"\"\"\n",
    "    Streaming chatbot using Qwen2-1.5B-Instruct and Hugging Face TextStreamer.\n",
    "\n",
    "    Parameters:\n",
    "        messages (list): List of message dicts (system, user, assistant roles).\n",
    "        max_new_tokens (int): Maximum tokens to generate.\n",
    "        temperature (float): Sampling temperature.\n",
    "        top_k (int): Top-k sampling.\n",
    "        top_p (float): Top-p sampling.\n",
    "\n",
    "    Output:\n",
    "        Streams assistant's reply to stdout.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer([prompt_text], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            streamer=streamer\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5e5e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Qwen Streaming Chatbot**\n",
      "Type 'exit' to quit.\n",
      "\n",
      "User:  Hello! Tell me about your expertise in about 100 words\n",
      "Assistant:  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, my main expertise is in natural language processing and understanding human language. I am designed to analyze and process text data, including text from online sources, chat conversations, and other written material, to extract meaning and summarize information. My ability to understand context and relationships between sentences makes me useful for a wide range of tasks, including answering questions, generating responses to prompts, and even providing explanations for complex concepts or ideas.\n",
      "\n",
      "User:  exit\n",
      "Goodbye..\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"**Qwen Streaming Chatbot**\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "    chat_history = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        print(\"User: \", user_input)\n",
    "\n",
    "        if user_input.strip().lower() in ['exit', 'quit']:\n",
    "            print(\"Goodbye..\")\n",
    "            break\n",
    "\n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        print(\"Assistant: \", end=\" \", flush=True)\n",
    "        qwen_chatbot_stream(chat_history)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4213f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
